import os
from dotenv import load_dotenv
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from pathlib import Path

def run_bronze_ingestion():
    # 1. Kh·ªüi t·∫°o Spark Session (B·∫Øt bu·ªôc khi ch·∫°y file .py)
    spark = SparkSession.builder \
        .appName("BronzeIngestion") \
        .config("spark.jars.packages", "com.microsoft.azure:azure-eventhubs-spark_2.12:2.3.18") \
        .getOrCreate()

    sc = spark.sparkContext

    # 2. C·∫•u h√¨nh (N√™n d√πng bi·∫øn m√¥i tr∆∞·ªùng ƒë·ªÉ b·∫£o m·∫≠t thay v√¨ d√°n tr·ª±c ti·∫øp)
    current_dir = Path(__file__).parent 

# T√¨m file .env ·ªü th∆∞ m·ª•c cha (th∆∞ m·ª•c g·ªëc d·ª± √°n)
    env_path = current_dir.parent / '.env'

    load_dotenv(dotenv_path=env_path)

# L·∫•y bi·∫øn t·ª´ .env (t√™n bi·∫øn th·ªëng nh·∫•t nh∆∞ ta ƒë√£ s·ª≠a ·ªü c√°c b∆∞·ªõc tr∆∞·ªõc)
    CONNECTION_STR = os.getenv('EVENT_HUBS_CONNECTION_STRING')
    BRONZE_OUTPUT_PATH = os.getenv('BRONZE_OUTPUT_PATH')
    CHECKPOINT_PATH = os.getenv('CHECKPOINT_PATH')

    # 3. C·∫•u h√¨nh Event Hubs
    # L∆∞u √Ω: L·ªánh sc._jvm ch·ªâ ho·∫°t ƒë·ªông khi c√≥ k·∫øt n·ªëi Databricks Connect
    eh_connection_string = sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(
        CONNECTION_STR + ";EntityPath=transactions"
    )
    
    ehConf = {
      'eventhubs.connectionString' : eh_connection_string
    }

    # 4. Pipeline x·ª≠ l√Ω
    print("üöÄ ƒêang b·∫Øt ƒë·∫ßu lu·ªìng ƒë·ªçc t·ª´ Event Hubs...")
    raw_df = spark.readStream \
      .format("eventhubs") \
      .options(**ehConf) \
      .load()

    decoded_df = raw_df.select(col("body").cast("string").alias("transaction_data"))

    # 5. Ghi d·ªØ li·ªáu
    query = decoded_df.writeStream \
        .format("delta") \
        .outputMode("append") \
        .option("checkpointLocation", CHECKPOINT_PATH) \
        .start(BRONZE_OUTPUT_PATH)

    query.awaitTermination() # Gi·ªØ cho script ch·∫°y li√™n t·ª•c

if __name__ == "__main__":
    run_bronze_ingestion()